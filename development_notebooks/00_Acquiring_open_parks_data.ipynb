{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31652ad5",
   "metadata": {},
   "source": [
    "# Downloading and acquiring datasets\n",
    "\n",
    "This notebook lays out how to download or access various different datasets.\n",
    "\n",
    "Note that in cases of a one-off download, we may provide manual download instructions instead of API calls.\n",
    "\n",
    "**Note: to run the other notebooks, you must ensure you have downloaded the data listed here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e864fd9",
   "metadata": {},
   "source": [
    "## Direct/manual downloads\n",
    "\n",
    "Currently, both the Local Authority District (LAD) and Ordnance Survey Open Greenspace datasets are downloaded manually.\n",
    "\n",
    "These can be accessed here:\n",
    "\n",
    "- [May 2024 Local Authority District (LAD) data](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://geoportal.statistics.gov.uk/datasets/ons::local-authority-districts-may-2024-boundaries-uk-bfe-2/about&ved=2ahUKEwi83bT-pJGOAxWBUUEAHSA-NZsQFnoECAoQAQ&usg=AOvVaw3nYML7UR9GdX1gnUYTH8uz)\n",
    "- [OS Open Greenspace Data](https://osdatahub.os.uk/downloads/open/OpenGreenspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470209a4",
   "metadata": {},
   "source": [
    "## In-script downloads\n",
    "\n",
    "The Open Street Map data is downloaded within the [01 Processing parks data notebook](01_Processing_parks_data.ipynb). The area downloaded can be altered by changing the LAD code used (in this case, Bradford)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba10699",
   "metadata": {},
   "source": [
    "## Webscraping park data\n",
    "\n",
    "The below webscraping script and subsequent processing code was developed by Fran Pontin, in order to pull information about Bradford city parks from the official [Bradford District Parks website](https://bradforddistrictparks.org/).\n",
    "\n",
    "[The Bradford District Parks website](https://bradforddistrictparks.org/) provides data on formally recognised parks and greenspaces in Bradford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12483b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# Function to get all links from a single page\n",
    "def get_links_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True, rel='bookmark')]\n",
    "    return links\n",
    "\n",
    "# Base URL of the webpage to scrape\n",
    "base_url = 'https://bradforddistrictparks.org/park/page/'\n",
    "\n",
    "# Initialize an empty list to store all links\n",
    "all_links = []\n",
    "\n",
    "# Loop through all pages (assuming there are 10 pages)\n",
    "for page_num in range(1, 11):\n",
    "    page_url = f'{base_url}{page_num}/'\n",
    "    links = get_links_from_page(page_url)\n",
    "    all_links.extend(links)\n",
    "\n",
    "parks_data = []\n",
    "\n",
    "# Loop through each park link and extract information\n",
    "for link in all_links:\n",
    "    # Send a GET request to the park webpage\n",
    "    response = requests.get(link)\n",
    "    \n",
    "    # Parse the park webpage content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "\n",
    "    # Extract park name\n",
    "    park_name = soup.find('h1', class_='entry-title').text.strip() if soup.find('h1', class_='entry-title') else 'NA'\n",
    "\n",
    "    # Extract location\n",
    "    location = soup.find('li', class_='location').text.strip() if soup.find('li', class_='location') else 'NA'\n",
    "    \n",
    "    # Extract opening hours\n",
    "    opening_hours = soup.find('li', class_='calendar').text.strip() if soup.find('li', class_='calendar') else 'NA'\n",
    "    \n",
    "    # Find the unordered list containing the opening hours and location\n",
    "    parent_ul = soup.find('li', class_='calendar').find_parent('ul') if soup.find('li', class_='calendar') else None\n",
    "    \n",
    "    # Find the next unordered list after the parent unordered list\n",
    "    next_ul = parent_ul.find_next('ul') if parent_ul else None\n",
    "    \n",
    "    # Extract elements that appear in the next unordered list\n",
    "    elements = []\n",
    "    if next_ul:\n",
    "        for li in next_ul.find_all('li'):\n",
    "            elements.append(li.text.strip())\n",
    "    else:\n",
    "        elements.append('NA')\n",
    "    \n",
    "    # Extract latitude and longitude from data attributes\n",
    "    map_element = soup.find('div', {'data-lat': True, 'data-lng': True})\n",
    "    latitude = map_element['data-lat'] if map_element else 'NA'\n",
    "    longitude = map_element['data-lng'] if map_element else 'NA'\n",
    "    \n",
    "    # Append the extracted data to the parks_data list\n",
    "    parks_data.append({\n",
    "        'Park Name': park_name,\n",
    "        'Park URL': link,\n",
    "        'Location': location,\n",
    "        'Opening Hours': opening_hours,\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Park features': elements\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the parks_data list\n",
    "parks_df = pd.DataFrame(parks_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b532a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31767/2812734370.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  parks_df['Latitude'].replace('NA', np.nan, inplace=True)\n",
      "/tmp/ipykernel_31767/2812734370.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  parks_df['Longitude'].replace('NA', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "parks_df['Latitude'].replace('NA', np.nan, inplace=True)\n",
    "parks_df['Longitude'].replace('NA', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5cbd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf =gpd.GeoDataFrame(parks_df,geometry=gpd.points_from_xy(parks_df.Longitude, parks_df.Latitude), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c0af4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_park_features =list(parks_gdf['Park features'].explode().unique())\n",
    "# Create columns for each unique item and populate with 1 or 0\n",
    "for feature in unique_park_features:\n",
    "    parks_gdf[feature] = parks_gdf['Park features'].apply(lambda x: 1 if feature in x else 0)\n",
    "\n",
    "parks_gdf['Park features str']=parks_gdf['Park features'].astype(str)\n",
    "parks_gdf['Park features str'] =parks_gdf['Park features str'].str.replace('[','').str.replace(']','').str.replace(\"'\",\"\").str.replace('\"',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e07d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf.drop(columns='Park features', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18d763",
   "metadata": {},
   "source": [
    "### Save out datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76227760",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save out datasets\n",
    "\n",
    "parks_df.to_csv('../data/data_downloads/bradford_scraped_parks_data.csv', index=False)\n",
    "parks_gdf.to_file('../data/data_downloads/bradford_district_parks.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03448fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safer-parks-env-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
